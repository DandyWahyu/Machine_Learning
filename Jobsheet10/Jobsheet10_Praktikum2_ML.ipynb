{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DandyWahyu/Machine_Learning/blob/main/Jobsheet10/Jobsheet10_Praktikum2_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Nama : Dandy Wahyu Syahputra\n",
        "##Kelas : TI-3A\n",
        "##Mata Kuliah : Machine Learning"
      ],
      "metadata": {
        "id": "X3kmTrpXJLIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "i2WNzdujIwLj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Dataset Shakespeare\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj7ywO3AI6CI",
        "outputId": "3cb35027-f407-4508-a431-7f471f538664"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2U-itL2JQCu",
        "outputId": "2dfd109c-1c93-438a-e96c-504263a84ba2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0jbxX7dJXJA",
        "outputId": "ed781c8e-871b-43b9-d593-9b177ec3ae65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntG3SO0AJZbl",
        "outputId": "9416e87d-3d81-4282-bfcf-bea5671c51f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Olah Teks\n",
        "# Vectorize Teks\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoQKs-kpJa_m",
        "outputId": "18a8102b-c0b2-44be-928f-bc9d5185147b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sekarang buat tf.keras.layers.StringLookup layer:\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "r0f1kQA8JnDS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perintah diatas mengconvert token menjadi id\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeubiVzZJwAe",
        "outputId": "f6143e2c-7499-44c0-b1af-3ef92c1bce5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kosakata asli yang dihasilkan dengan diurutkan(set(teks)) gunakan metode get_vocabulary()\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "wTqPQsYZJ2fa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqhSjEADJ-Np",
        "outputId": "e39ccdcb-6a93-49cf-d239-2b44b8332318"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Anda dapat menggunakan tf.strings.reduce_join untuk menggabungkan kembali karakter menjadi string.\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM-nVHEMKEzp",
        "outputId": "b144da24-0a20-4159-cec2-363005e9bea2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "I0e5SeVVKRfo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi\n",
        "# mengonversi vektor teks menjadi aliran indeks karakter.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqTtRvgsKX0u",
        "outputId": "06b20d46-0ae6-4e48-dcb6-7d762a71c15c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "UE46kR_3Kn1I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "seq_length = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JiqA6JtKric",
        "outputId": "49485ac9-3920-4876-a2b4-46158a8254d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC1xJlO4KzDo",
        "outputId": "14357753-756f-4587-cf08-32e3f87c5786"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string:\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFv7uYcSK3_C",
        "outputId": "7a892e12-415e-4f89-eb9a-67c49e15ff73"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "Ndbwkp0iK8vg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKM4eWg6LOrg",
        "outputId": "c6f2ea0d-58d3-4d4f-b0be-91e17a1dc0de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat Batch Training\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OugjkBk7LWaB",
        "outputId": "4efca32a-e6d2-438f-a35e-8e4f78e4f13e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat Model\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "qyp0gNLjLb1f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "b6stBk5gLhJj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "XFpbnBNKLjXp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uji Model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FgszJmWLlgz",
        "outputId": "99b0814f-780c-40a4-d22f-27bb3e9ba99e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiBZywFFLpbI",
        "outputId": "febe349c-62cd-47ae-f0f5-943207f85e70"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "10WQVuj8LsLK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coJkQkWFLunh",
        "outputId": "17a5e390-65b9-46df-84cd-6962d860bae6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16, 42, 18, 28, 51, 26, 34, 13, 60,  3, 57, 24, 62,  2,  4, 46, 15,\n",
              "       43, 35, 20, 38, 44, 28, 30, 64, 32, 47, 62, 27, 14, 42, 56, 11, 50,\n",
              "       47, 55, 50, 38, 38, 21, 27,  0,  9, 22, 15, 60, 20, 63, 22,  4, 58,\n",
              "       22, 44, 47, 22, 51, 20, 11, 12, 64, 43, 30, 48, 20, 38, 63, 45, 20,\n",
              "       64,  2, 57, 52, 12, 38, 26, 58,  5, 45, 26,  7, 48, 37, 28, 60, 62,\n",
              "       21, 44, 28, 38, 28,  2,  4, 36, 42,  6, 23,  2, 17,  9, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdCKRaj1L0g-",
        "outputId": "97ad2b0a-8ed1-4e21-e62a-5342317834d8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'yet I love him.\\n\\nDUKE OF YORK:\\nMake way, unruly woman!\\n\\nDUCHESS OF YORK:\\nAfter, Aumerle! mount thee '\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"CcEOlMU?u!rKw $gBdVGYeOQyShwNAcq:khpkYYHN[UNK].IBuGxI$sIehIlG:;ydQiGYxfGy rm;YMs&fM,iXOuwHeOYO $Wc'J D.M\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "7l-7R8rYL4rE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgzZBzMjL8Vw",
        "outputId": "e00546d0-5ed0-4ac0-f56d-27b9eaf67705"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190031, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKlnNaMOL_C1",
        "outputId": "4560e88b-6c76-47df-c775-db0fc1eea5e0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.02484"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "UivwcqLTMCHg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi Checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "RXUu7UoSME81"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lakukan Proses Training\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "377zZLuMMJwr",
        "outputId": "786c5829-1bc1-496d-ef0f-e0b13c64dff1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 53ms/step - loss: 2.7003\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 50ms/step - loss: 1.9867\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 1.7043\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.5440\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.4452\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.3781\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 1.3255\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.2807\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.2402\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.2010\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.1608\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.1183\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.0746\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.0298\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.9804\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.9300\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.8779\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.8244\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.7734\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.7252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Berikut ini membuat prediksi satu langkah\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "cWCP7LzJMSkE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "mPgcmQwFMbT-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Karena sedikitnya jumlah epoch pelatihan, model belum belajar membentuk kalimat runtut.\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YX4AtExMehc",
        "outputId": "9d4d940c-9899-4e3a-9a3e-8c384f99ba78"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Care not to be a ramber hence power in hell,\n",
            "The whilst thyself needs will not they attend\n",
            "To crush at our request; my heart is so\n",
            "That thou, his fact, truly, my dear soul!\n",
            "Yet, see your wrongs revenge\n",
            "But only therefore either join'd\n",
            "With more respected fence issued:\n",
            "Profaned, the envious, so time-profaned\n",
            "Were five-fully so like an Juliet:\n",
            "'ere they put thy speech and proceedings from Angelo,\n",
            "Which seem to all, that rotten as much use\n",
            "In secrets to the willow sake; and therefore fare your womb,\n",
            "Is never more no chain heartes the cunk.\n",
            "Ready with your credit, my Lord Waster,\n",
            "But thou art taken: let me company,\n",
            "Since thou art poor'st that ever here lies her by;\n",
            "I pray she, 'tis haste thee and thy foot,\n",
            "Nor can my father with all endering.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Bliss much noble gentleman in this, go thither\n",
            "Of our good city for a kingdom's word:\n",
            "Advance these fearful where at movesty,\n",
            "If hear no help, my friends at then, is it done,\n",
            "And so be crown'd Norfolk. Then is so tender,\n",
            "Those you o \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1971843242645264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model menghasilkan 5 keluaran dalam waktu yang hampir sama dengan waktu yang dibutuhkan untuk menghasilkan 1 keluaran di atas.\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPi6svLMwbJ",
        "outputId": "536201fa-619c-4fbe-f5ed-25a9e98999b8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThat's assurance.\\n\\nCAMILLO:\\nAy, good, speak so.\\n\\nDUCHESS OF YORK:\\nArt thou gone??\\n\\nBRUTUS:\\nThere wants upon him.\\n\\nFourth Citizen:\\nYou said 'Ay's worth and framed into the marriage.\\n\\nBUCKINGHAM:\\nGod-purge us through the infant from her truth,\\nDetermine there be order study,\\nTake thou the cate whether that beggar been aired good.\\nWhat face your highness to their speech?\\n\\nBENVOLIO:\\nIs the worse? Hereigh wrongful ill.\\n\\nROMEO:\\nIs it even now our head i' the sword;\\nThat could not going the senate will pierce from me\\nThe friar and to be on here to maintain with words?\\n\\nQUEEN ELIZABETH:\\nThanks, dear complaint?\\n\\nCitizens:\\nThink'st thou not, God, is good to hear.\\nGo me, good friend Northumberland, I wish\\nAn oath thou stand'st control of such company.\\n\\nROMEO:\\nTut, doubt not to visit the gors, woos, a\\ndifference; he should hear me lets, but how I\\nwrong to do two opent the seconds.\\n\\nAUTOLYCUS:\\nI will, sir, my lord, and I'll be chosed\\nas theirs. Theirs, lords,\\nA man of majow her yet success.\\n\\nCLAUD\"\n",
            " b\"ROMEO:\\nMy love, an unshe tale every.\\n\\nGentleman:\\nAy.\\n\\nLUCENTIO:\\nA little age, in whose names appeart,\\nAgainst a time of death.\\n\\nBENVOLIO:\\nAy, with my holy pager.\\n\\nMERCUTIO:\\nThou'ldst thou diest thy nose business?\\n\\nQUEEN MARGARET:\\nBeek look!--How now! what news?\\n\\nRATCLIFF:\\nDost thou hear? a hall. Ghom me, good den,\\nWhat makes your mojesty have wrong you thanks.\\n\\nFRIAR JOHN:\\nHere: but he is dead; 'tis not my chance,\\nTo breathe this face of him of compassion will why.\\nI'll wear thee prospery? Commend me fairer, I saw my case\\nThan like an oath to fly thee?\\n\\nDUKE VINCENTIO:\\nThis is the what they were a fortive intercises\\nOf what with an air with the seevy forth.\\n\\nYORK:\\nHere's my more lusty any horse for better.\\n\\nAUTOLYCUS:\\nAy, and be it your worship.\\n\\nAUFIDIUS:\\nO letters, those fellows\\nStrike all this, that have betreats that\\ntake it.\\n\\nANTONIO:\\nAre therefore do't.\\n\\nBRUTUS:\\nIn fellow, give you now.\\n\\nAUTOLYCUS:\\nIf they are possible.\\n\\nGRUMIO:\\nNo, sir: yet often is coming from thee take.\\n\\nQUEEN MARGA\"\n",
            " b\"ROMEO:\\nThus hast thou, without, how long have enter'd lover.\\n\\nSecond Soldier:\\nI shall not go on: as he not quake\\nThe state of honour to betix them seal,\\nThat whetself enjoy it, I did fell.\\n\\nKING HENRY VI:\\nFor which I wot not, but to ride;\\nWhich well and very weary with my head\\nAs is a twint shed, and in the world shook her,\\nAs give him good means to come by with wings.\\nNow is gone to Edna, who, as ho royal best\\nWarn'd thy name to Edward, but one for thee.\\n\\nGLOUCESTER:\\n\\nKING EDWARD IV:\\nCan as a burthen, ach, visit be shorter.\\n\\nHENRY BOLINGBROKE:\\nStay well ere you gont.\\n\\nCLAUDIO:\\nUpon record, my gracious lord,\\nYou must to do't with Pompey to leave.\\nAnd save your little too, to jest! Therefore, I,\\nShe says so much about her; now it should be behind;\\nFrom whence, thou liest, maliged to a show\\nRepose him in almost barren, if our benche;\\nAnd, piteous hook!--the lady is gone,\\nAnd I will not have my wo'der:\\nThe news wrong, it is some thing to friendly\\ntherefore: all but the state days which thou kno\"\n",
            " b\"ROMEO:\\nWhy, couragot mercy.\\n\\nDUKE VINCENTIO:\\nBe call'd my cobse.\\n\\nRIVERS:\\nYea, and my nurse, this is Benught sue.\\nThou is the postern of your ghouts,\\nMore heelfully dispenses; O, her far hence,\\nTo help of marriage be with you; reverence or a very hear'd\\nWho tongue is not quenith than no hare.\\nD speeder, let's;\\nAnd I am prey to-dancing to myself,\\nMake pale like anchors shall believe\\nThat's like an idle that will; these\\nseeks before a word: young word to telch them\\nIf she do bite a place where care to such a power?\\n\\nCOMINIUS:\\nHear me a brother, if thou do:'re i' the pries,\\nAnd blown our soldiers yours; but let my flesh,\\nUrle in all that hope is less\\nThan she, the Duke of Norfolk,--\\nThou hadst been pusiness, like to meet\\nBy the fire, the former life, for decret\\nHe was a king, call'd by the pack of him.\\n\\nFirst Gentleman:\\nIf it be so. Edward, my liege, ready a mystery.\\n\\nPage:\\nThe session since we call'd for the city; I'll after thee\\nAll all when they are our fault us.\\n\\nPOLIXENES:\\nSir, you woo'd h\"\n",
            " b\"ROMEO:\\nTut, turn your good courses that hath more you?\\n\\nARCHBISHOP OF CARLOS:\\nEvermore what we will be keen?\\n\\nPedant:\\nSir, at your chair a needy as these?\\n\\nDUKE OF AUMERLE:\\nHere, Sir Lucio, Gilvet with no other, but it is\\nThe seadled of the hearers that you love.\\n\\nLUCENTIO:\\nSeil'd by the loss,\\nConform'd conceive a lion in resolute.\\n\\nQUEEN MARGARET:\\nAy, as they mark up that was much directitude,\\nIftering the grace I to my wife, I'll give thee mine.\\n\\nHENRY PERCY:\\nNay, my gracious lord, I say to her for him\\nTo the Ten Yestenger: thou art a fool!\\nBut you take each other tears to wear a debt.\\n\\nS Choed:\\nNo love be ready, lo, here's my nunce to grant him.\\n\\nupon:\\nGo thou, and thou didst kiss my son! and let him be fuest:\\nWith varined from my weather selt it for\\nmy heart my master brother?\\n\\nISABELLA:\\nPeace.\\n\\nSEBASTIAN:\\nYour metesy hands.\\nBut what's thy name?\\n\\nPETRUCHIO:\\nHow buried not so: I daughter, and I trow like him\\nAnd pluck our general: a prectivate frame,\\nTo virtue with them come.\\n\\nBIONDELLO:\\n\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.216219663619995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekspor Model Generator\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNefko9tMzel",
        "outputId": "1119c8a8-569c-48cb-9f7c-264e45657a49"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7943885e8610>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The strength narry fast; and I say to your pains,\n",
            "That I may much else friend at Pomfret-harbels, I\n"
          ]
        }
      ]
    }
  ]
}